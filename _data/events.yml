events:
  - date: 2025-07-08 10:00
    speaker: 李吉星博士
    title: 大语言模型与人脑语言加工的对齐机制
    institution: 香港城市大学
    permalink: 2025-07-Li-Jixing
    photo: jixing.png
    status: past
    zoom_link: https://cuhk.zoom.us/j/95891053788?pwd=lWyxDJW0hy9Bkjaubfo9pmUz9qUh7h.1
    zoom_id: 958 9105 3788
    zoom_password: 213165
    abstract: |
       近年来，大语言模型与人脑对齐的研究不断深入，不仅加深了我们对人类大脑语言处理机制的理解，也为未来构建更类脑的人工智能系统提供了重要启示。在本次报告中，我将首先介绍一项基于fMRI数据的相关性研究。研究表明，与指令微调相比，模型规模的扩展更显著地增强了其对人脑语言信息的表征能力。随后，我将分享一项因果性研究，通过对大语言模型的特定模块或参数进行系统性屏蔽与扰动，来观察其在语言任务中的功能变化。结果显示，经过“损伤”处理的模型更容易表现出类似韦尼克失语症的语义障碍，而模拟布洛卡失语症中以句法受损为特征的语言障碍则相对困难。这一发现揭示了当前语言模型在模块化语言加工方面与人脑之间的异同。
    bio: |
      李吉星现为香港城市大学语言学系助理教授。她于2019年获得康奈尔大学语言学博士学位，于2022年在纽约大学阿布扎比分校的语言神经科学实验室完成博士后研究。她的研究结合自然语言处理模型，探索人脑的语言加工机制，综合语言学、心理学、认知神经科学及自然语言处理等多个学科领域。其研究成果已发表于Nature Computational Science、 eLife、Journal of Neuroscience、Scientific Data、Imaging Neuroscience等期刊，以及 ACL 等自然语言处理顶级会议。担任Communications Psychology编委会成员。
    outline: |
      1. 讲座内容
      2. 问答环节
    resources: |
      - [预读论文](https://www.biorxiv.org/content/10.1101/2024.08.15.608196v4)
    video_link: https://cuhk.zoom.us/rec/share/OudBZBZxGjdd9NXkUjyTKiLMTkuiTkkhHLqJeCuCXBtshbOyTK7_QNr66eojRxrw.FHiQYRI-FGh_4Ozo?startTime=1751940008000
    video_password: Cf7.5C+&
  
  - date: 2025-07-28 10:00
    speaker: 于劭赟博士
    title: 大语言模型遇上阅读中的大脑
    institution: 香港中文大学
    permalink: 2025-07-Yu-Shaoyun
    photo: shaoyun.jpg
    status: past
    zoom_link: https://cuhk.zoom.us/j/95891053788?pwd=lWyxDJW0hy9Bkjaubfo9pmUz9qUh7h.1
    zoom_id: 958 9105 3788
    zoom_password: 213165
    abstract: |
       词语预测一直是大语言模型的核心预训练任务，并推动了当前模型的成功。然而，这一任务是否足以支撑大语言模型迈向通用人工智能，仍是学术界广泛讨论的问题。事实上，在人类的语言使用过程中，词语预测远非唯一的认知机制。这在以阅读为代表的语篇加工中尤为显见：我们不仅会预测词语，还需要在句子之上的层面理解语义的连贯性及逻辑关系。本次报告中，我将首先介绍在模型训练中引入句子层级预测任务的尝试，并探讨其对模型与人脑对齐的影响。此外，我还将讨论，虽然大脑中的语义表征同样依赖上下文信息，但其与模型所使用的上下文窗口机制可能有本质不同。最后，模型与大脑的对齐程度也会因被试个体而异，这提示我们本领域研究中仍有很多未知有待探索。
    bio: |
      于劭赟现为香港中文大学语言学及现代语言系研究助理教授。他于2021年获得名古屋大学人文学博士学位，于2025年在香港理工大学脑、语言及计算实验室完成博士后研究。他的研究兴趣涵盖人类语言的多个基本组成部分，从基础的句法结构、具身认知，到更高层次的语篇理解。他结合计算建模、神经影像和行为实验的方法，以跨学科的视角研究这些课题。其研究成果已发表于Science Advances、 Brain and Language、Frontiers in Psychology、Language Sciences等期刊。
    outline: |
      1. 讲座内容
      2. 问答环节
    resources: |
      - [预读论文](https://www.science.org/doi/10.1126/sciadv.adn7744)
    video_link: https://cuhk.zoom.us/rec/share/YTNiKs-gssSIdDnuoI4fUtRwcP8KlGGOAmB2USgc7nIvL3yOW7xQtaAQpMYBbLRP.Rk9C0_3vf7FSmpq4?startTime=1753668128000
    video_password: MVZ7eW#u

  - date: 2025-09-01 10:00
    speaker: 王浩丞
    title: 语音文本大模型可以预测人脑自然语言活动
    institution: 普林斯顿大学
    permalink: 2025-09-Wang-Haocheng
    photo: haocheng.jpg
    status: active
    zoom_link: https://cuhk.zoom.us/j/95891053788?pwd=lWyxDJW0hy9Bkjaubfo9pmUz9qUh7h.1
    zoom_id: 958 9105 3788
    zoom_password: 213165
    abstract: |
      人类在日常对话中可以毫不费力地在听和说之间无缝切换。那么，人脑的神经机制是如何支持自然语言理解和生成的呢？在本研究中，我们采集了一个大规模的皮层脑电图（ECoG）数据集。被试者在长达一周的时间里，与病房中的家人、朋友、医生和医护人员等进行了开放式真实对话。我们从中提取出了总计约一百个小时（五十万词）的语言理解和生成的脑电图数据，以及同步的语音和文本信息。近年来快速发展的大语言模型为我们提供了一个分析自然数据的计算框架，因为这些模型能够捕捉到现实世界中语言的多样性和丰富的语境。我们使用了一个多模态语音转文本模型 Whisper，从中提取出声学、语音和语言的嵌入向量（vector embedding），并构建了一系列编码（encoding）模型，利用这些嵌入向量来预测大脑活动。结果显示，我们的编码模型能够准确预测语言理解和生成的时间序列，并且模型内部的处理层级与人类语言系统的层级结构相吻合。此外，基于嵌入向量的编码模型在预测大脑活动方面优于传统的符号模型。这些发现表明，大语言模型可以成为一个统一的计算框架，用于预测人脑语言理解和生成的时间进程、空间分布和层级结构。
    bio: |
      王浩丞现于普林斯顿大学Hasson Lab就读心理学博士。他拥有生物学，数学，和计算神经科学学士学位。他目前使用大规模自然数据集和深度学习模型来研究人脑如何理解和生成语言。他的研究兴趣包括人脑语言系统与记忆系统的交流，脑对脑交流，和语言习得等。其研究成果已发表于Nature Human Behavior, Nature Communications, Nature Computational Science, eLife等期刊。
    outline: |
      1. 讲座内容
      2. 问答环节
    resources: |
      - [预读论文](https://doi.org/10.1038/s41562-025-02105-9)
    