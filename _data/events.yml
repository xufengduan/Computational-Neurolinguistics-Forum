events:
  - date: 2025-07-08 10:00
    speaker: 李吉星博士
    title: 大语言模型与人脑语言加工的对齐机制
    institution: 香港城市大学
    permalink: 2025-07-Li-Jixing
    photo: jixing.png
    status: past
    zoom_link: https://cuhk.zoom.us/j/95891053788?pwd=lWyxDJW0hy9Bkjaubfo9pmUz9qUh7h.1
    zoom_id: 958 9105 3788
    zoom_password: 213165
    abstract: |
       近年来，大语言模型与人脑对齐的研究不断深入，不仅加深了我们对人类大脑语言处理机制的理解，也为未来构建更类脑的人工智能系统提供了重要启示。在本次报告中，我将首先介绍一项基于fMRI数据的相关性研究。研究表明，与指令微调相比，模型规模的扩展更显著地增强了其对人脑语言信息的表征能力。随后，我将分享一项因果性研究，通过对大语言模型的特定模块或参数进行系统性屏蔽与扰动，来观察其在语言任务中的功能变化。结果显示，经过“损伤”处理的模型更容易表现出类似韦尼克失语症的语义障碍，而模拟布洛卡失语症中以句法受损为特征的语言障碍则相对困难。这一发现揭示了当前语言模型在模块化语言加工方面与人脑之间的异同。
    bio: |
      李吉星现为香港城市大学语言学系助理教授。她于2019年获得康奈尔大学语言学博士学位，于2022年在纽约大学阿布扎比分校的语言神经科学实验室完成博士后研究。她的研究结合自然语言处理模型，探索人脑的语言加工机制，综合语言学、心理学、认知神经科学及自然语言处理等多个学科领域。其研究成果已发表于Nature Computational Science、 eLife、Journal of Neuroscience、Scientific Data、Imaging Neuroscience等期刊，以及 ACL 等自然语言处理顶级会议。担任Communications Psychology编委会成员。
    outline: |
      1. 讲座内容
      2. 问答环节
    resources: |
      - [预读论文](https://www.biorxiv.org/content/10.1101/2024.08.15.608196v4)
    video_link: https://cuhk.zoom.us/rec/share/OudBZBZxGjdd9NXkUjyTKiLMTkuiTkkhHLqJeCuCXBtshbOyTK7_QNr66eojRxrw.FHiQYRI-FGh_4Ozo?startTime=1751940008000
    video_password: Cf7.5C+&
  
  - date: 2025-07-28 10:00
    speaker: 于劭赟博士
    title: 大语言模型遇上阅读中的大脑
    institution: 香港中文大学
    permalink: 2025-07-Yu-Shaoyun
    photo: shaoyun.jpg
    status: past
    zoom_link: https://cuhk.zoom.us/j/95891053788?pwd=lWyxDJW0hy9Bkjaubfo9pmUz9qUh7h.1
    zoom_id: 958 9105 3788
    zoom_password: 213165
    abstract: |
       词语预测一直是大语言模型的核心预训练任务，并推动了当前模型的成功。然而，这一任务是否足以支撑大语言模型迈向通用人工智能，仍是学术界广泛讨论的问题。事实上，在人类的语言使用过程中，词语预测远非唯一的认知机制。这在以阅读为代表的语篇加工中尤为显见：我们不仅会预测词语，还需要在句子之上的层面理解语义的连贯性及逻辑关系。本次报告中，我将首先介绍在模型训练中引入句子层级预测任务的尝试，并探讨其对模型与人脑对齐的影响。此外，我还将讨论，虽然大脑中的语义表征同样依赖上下文信息，但其与模型所使用的上下文窗口机制可能有本质不同。最后，模型与大脑的对齐程度也会因被试个体而异，这提示我们本领域研究中仍有很多未知有待探索。
    bio: |
      于劭赟现为香港中文大学语言学及现代语言系研究助理教授。他于2021年获得名古屋大学人文学博士学位，于2025年在香港理工大学脑、语言及计算实验室完成博士后研究。他的研究兴趣涵盖人类语言的多个基本组成部分，从基础的句法结构、具身认知，到更高层次的语篇理解。他结合计算建模、神经影像和行为实验的方法，以跨学科的视角研究这些课题。其研究成果已发表于Science Advances、 Brain and Language、Frontiers in Psychology、Language Sciences等期刊。
    outline: |
      1. 讲座内容
      2. 问答环节
    resources: |
      - [预读论文](https://www.science.org/doi/10.1126/sciadv.adn7744)
    video_link: https://cuhk.zoom.us/rec/share/YTNiKs-gssSIdDnuoI4fUtRwcP8KlGGOAmB2USgc7nIvL3yOW7xQtaAQpMYBbLRP.Rk9C0_3vf7FSmpq4?startTime=1753668128000
    video_password: MVZ7eW#u

  - date: 2025-09-01 10:00
    speaker: 王浩丞
    title: 多模态语言文本大模型可以全面预测人脑自然语言活动
    institution: 香港中文大学
    permalink: 2025-09-Wang-Haocheng
    photo: haocheng.jpg
    status: active
    zoom_link: https://cuhk.zoom.us/j/95891053788?pwd=lWyxDJW0hy9Bkjaubfo9pmUz9qUh7h.1
    zoom_id: 958 9105 3788
    zoom_password: 213165
    abstract: |
       本研究提出了一个统一的计算框架，该框架将声学、言语及词级语言结构联系起来，旨在探究人类大脑中日常对话的神经基础。研究中，我们采用皮层脑电图（electrocorticography）记录了参与者进行开放式真实对话时，长达 100 小时的言语生成与理解过程中的神经信号。我们从多模态语音转文本模型（Whisper）中提取了低层级声学嵌入、中间层级言语嵌入和语境化词嵌入，并构建了编码模型，通过线性映射将这些嵌入与言语生成及理解过程中的大脑活动关联起来。值得注意的是，对于未参与模型训练的数小时新对话，该模型仍能准确预测语言处理层级中每个层级的神经活动。模型的内部处理层级与负责言语和语言处理的皮层层级相匹配：感觉和运动区域与模型的言语嵌入更契合，而高层级语言区域则与模型的语言嵌入更匹配。Whisper 模型能够捕捉到言语生成中词汇发音前 “语言到言语” 的编码时间序列，以及言语理解中发音后 “言语到语言” 的编码时间序列。相比符号模型，该模型习得的嵌入在捕捉支持自然言语和语言的神经活动方面表现更优。这些发现为研究范式的转变提供了支持，即转向采用统一计算模型来捕捉现实对话中言语理解与生成的完整处理层级。
    bio: |
      王浩丞是普林斯顿大学的在读博士生。他拥有生物学、数学和计算神经科学学士学位。他热衷于构建更精准的大脑计算模型，目前正专注于通过对比皮层脑电图（ECoG）数据与大型语言模型，来研究大脑中语义表征的特性。
    outline: |
      1. 讲座内容
      2. 问答环节
    resources: |
      - [预读论文](https://doi.org/10.1038/s41562-025-02105-9)
    